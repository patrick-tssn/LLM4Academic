# Open Source LLMs

#### English Community

| Open Source Repository                                                                                  | Base Language Model                                | Language         | Accelerate                                                                                          | Tuning                                                     |
| ------------------------------------------------------------------------------------------------------- | -------------------------------------------------- | ---------------- | --------------------------------------------------------------------------------------------------- | ---------------------------------------------------------- |
| [stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)                                            | LLaMA7B                                            | English          | [fsdp](https://huggingface.co/docs/accelerate/usage_guides/fsdp)                                       | Instruction Tuning                                         |
| [LLaMA-Adapter](https://github.com/ZrrSkywalker/LLaMA-Adapter)                                             | LlaMA                                              | English          |                                                                                                     | adapter                                                    |
| [alpaca-lora](https://github.com/tloen/alpaca-lora)                                                        | LLaMA7B                                            | English          | [peft](https://github.com/huggingface/peft), [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) | Instruction Tuning                                         |
| [Alpaca-CoT](https://github.com/PhoebusSi/Alpaca-CoT)                                                      | LLaMA7B                                            | Chinese, English | [peft](https://github.com/huggingface/peft), [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) | Instruction Tuning                                         |
| [Lit-LLaMA](https://github.com/Lightning-AI/lit-llama) *(Lightning)*                                    | LLaMA                                              | English          | [LoRA](https://github.com/microsoft/LoRA)                                                              | Instruction Tuning                                         |
| [Llama-X](https://github.com/AetherCortex/Llama-X)                                                         | LLaMA                                              |                  |                                                                                                     | Instruction Tuning, RLHF (possible)), etc.                 |
| [Dromedary](https://github.com/IBM/Dromedary)                                                              | LLaMA                                              | English          |                                                                                                     | Instruction Tuning (safety)                                |
| [PandaLM](https://github.com/WeOpenML/PandaLM)                                                             | LlaMA, BLOOM                                       | English          |                                                                                                     | Instruction Tuning, evaluation                             |
| [Open-Llama](https://github.com/s-JoL/Open-Llama)                                                          | Transformers-Llama                                 |                  |                                                                                                     | Pretraining, Tuning (possible), RLHF (possible)            |
| [OpenAlpaca](https://github.com/yxuansu/OpenAlpaca)                                                        | OpenLLaMA                                          | English          |                                                                                                     | Instruction Tuning                                         |
| [Dolly](https://github.com/databrickslabs/dolly), v2                                                       | GPT-J, EleutherAI pythia (v2)                      | English          |                                                                                                     | Insturction Tuning                                         |
| [Koala (EasyLM)](https://bair.berkeley.edu/blog/2023/04/03/koala/)                                         | LLaMA, GPT-J, OPT, RoBERTa                         |                  | JAX/Flax                                                                                            | Instruction Tuning                                         |
| [Vicuna (FastChat)](https://github.com/lm-sys/FastChat)                                                    | LLaMA13B                                           | English          | [fsdp](https://huggingface.co/docs/accelerate/usage_guides/fsdp)                                       | Instruction Tuning                                         |
| [LMFlow](https://github.com/OptimalScale/LMFlow)                                                           | LLaMA, GPT2, GPT-Neo, Galactica                    |                  | [deepspeed](https://www.deepspeed.ai/docs/config-json/)                                                | Instruction Tuning, Inference, Alignment Tuning (possible) |
| [Coati](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat)                               | LLaMA7B                                            | English          |                                                                                                     | Instruction Tuning, RLHF                                   |
| [DeepSpeed-Chat](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat)  | OPT, BLOOM, GPT-NEOX, GPTJ, GPT-NEO, GPT2, CODEGEN |                  |                                                                                                     | Instruction Tuning, RLHF                                   |
| [StableLM](https://github.com/Stability-AI/StableLM)                                                       | Transformer                                        | English          |                                                                                                     | Pretraining, Fine-tuning                                   |
| [LLM Foundary](https://github.com/mosaicml/llm-foundry)                                                    | Transformer-GPT                                    | English          |                                                                                                     | Pretraining, Fine-tuning                                   |
| [WizardLM](https://github.com/nlpxucan/WizardLM)                                                           | LLaMA                                              | English          |                                                                                                     | Instruction-tuning                                         |

#### Chinese Community

| Open Source Repository                                               | Base Language Model | Language              | Accelerate                                                               | Tuning                   |
| -------------------------------------------------------------------- | ------------------- | --------------------- | ------------------------------------------------------------------------ | ------------------------ |
| [骆驼 Luotuo-Chinese-LLM](https://github.com/LC1332/Luotuo-Chinese-LLM) | LLaMA7B             | Chinese               | [peft](https://github.com/huggingface/peft)                                 | Instruction Tuning       |
| [Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)   | LLaMA               | Chinese               |                                                                          | Instruction Tuning       |
| [Chinese-Vicuna](https://github.com/Facico/Chinese-Vicuna)              | LLaMA               | Chinese               | LoRA                                                                     | Instruction Tuning       |
| [白泽 Baize](https://github.com/project-baize/baize-chatbot)            | LLaMA               | Chinese               | [LoRA](https://github.com/microsoft/LoRA)                                   | Instruction Tuning       |
| [流萤 Firefly](https://github.com/yangjianxin1/Firefly)                 | BLOOM               | Chinese               | [LLMPruner](https://github.com/yangjianxin1/LLMPruner)                      | Instruction Tuning       |
| [BELLE](https://github.com/LianjiaTech/BELLE)                           | BLOOMZ-7B1-mt       | Chinese               |                                                                          | Instruction Tuning       |
| [TigerBot](https://github.com/TigerResearch/TigerBot)                   | BLOOM               | Chinese, English      | GPTQ                                                                     | Pretrain, Tuning         |
| [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)                       | GLM6B               | Chinese, English      | [p-tuning](https://github.com/THUDM/ChatGLM-6B/blob/main/ptuning/README.md) | Instruciton Tuning, RLHF |
| [InstructGLM](https://github.com/yanqiangmiffy/InstructGLM)             | ChatGLM-6B          | Chienese              | [deepspeed](https://www.deepspeed.ai/docs/config-json/)                     | Instruction Tuning       |
| [Phoenix](https://github.com/FreedomIntelligence/LLMZoo)                | LLaMA, Bloomz       | Chinese, Multilingual |                                                                          | Instruction Tuning       |
| [MOSS](https://github.com/OpenLMLab/MOSS)                               | Transformer         | Chinese, English      |                                                                          | Pretraining, Fine-Tuning |
| [baichuan](https://github.com/baichuan-inc/baichuan-7B)                 | Transformer         | Chinese, English      |                                                                          | Pretraining              |

#### References

- [LLM-Zoo](https://github.com/DAMO-NLP-SG/LLM-Zoo)
- [FindTheChatGPTer](https://github.com/chenking2020/FindTheChatGPTer)
- [Awesome-ALM](https://github.com/pbhu1024/awesome-augmented-language-model#action-and-plan)