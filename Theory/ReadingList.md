# LLM Reading

Table of Contents

- [Instruction Tuning](#instruction-tuning)
- [In Context Learning](#in-context-learning)
- [Chain of Thought](#chain-of-thought)

## Key Findings

| Title                                                               | Pub | Preprint                                                                                                                                                                                                                                                                                          | Supplementary       |
| ------------------------------------------------------------------- | --- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------- |
|          Emergent Abilities of Large Language Models                                                           |   TMLR 2022  |          [2206.07682](https://arxiv.org/abs/2206.07682)  <br />   ![Dynamic JSON Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fdac3a172b504f4e33c029655e9befb3386e5f63a%3Ffields%3DcitationCount&query=%24.citationCount&label=citation)                                                                                                                                                                                                                                                                                     |                   Google  |
| Training language models to follow instructions with human feedback |     | [2203.02155](https://arxiv.org/abs/2203.02155) <br />    ![Dynamic JSON Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fd766bffc357127e0dc86dd69561d5aeb520d6f4c%3Ffields%3DcitationCount&query=%24.citationCount&label=citation)  | InstructGPT, OpenAI |
| Language Models are Few-Shot Learners                               |     | [2005.14165](https://arxiv.org/abs/2005.14165)  <br />    ![Dynamic JSON Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6b85b63579a916f705a8e10a49bd8d849d91b1fc%3Ffields%3DcitationCount&query=%24.citationCount&label=citation) | GPT3, OpenAI        |
| Scaling Laws for Neural Language Models                             |     | [2001.08361](https://arxiv.org/abs/2001.08361) <br />   ![Dynamic JSON Badge](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fe6c561d02500b2596a230b341a8eb8b921ca5bf2%3Ffields%3DcitationCount&query=%24.citationCount&label=citation)   | scaling law, OpenAI |

## Instruction Tuning

Papers/blogs about Instruction Tuning, see [Instruction Tuning](InstructionTuning.md) for details.

> - Papers
> - Blogs
> - Reference

## In Context Learning

Papers about In Context Learning, see [In Context Learning](ICL.md) for details.

> - Papers
> - Reference

## Chain of Thought

Papers about Chain of Thought, see [Chain of Thought](CoT.md) for details.

> - Papers
> - Reference

## References

- [LLMSurvey](https://github.com/RUCAIBox/LLMSurvey), A collection of papers and resources related to Large Language Models
  - [survey](https://arxiv.org/abs/2303.18223)
- [LLMsPracticalGuide](https://github.com/Mooler0410/LLMsPracticalGuide), A curated list of practical guide resources of LLMs (LLMs Tree, Examples, Papers)
  - [survey](https://arxiv.org/abs/2304.13712), [blog](https://jingfengyang.github.io/gpt)
- [Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM), a curated list of Large Language Model
